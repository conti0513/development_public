以下に **個人用の作業手順（README_personal.md）** を作成しました。  
このファイルを `README_personal.md` として `gcp-sftp-transfer` リポジトリに保存すれば、今後の作業がスムーズになります。  

```markdown
# GCP → AWS SFTP 転送: 個人用作業手順

このドキュメントは **GCP Cloud Functions** を使用して **Google Cloud Storage (GCS) のファイルを AWS SFTP に自動転送** する作業手順をまとめたものです。

---

## **1. 環境セットアップ**
### **1.1 必要なツールのインストール**
```sh
# Google Cloud SDK（gcloud コマンド）
curl https://sdk.cloud.google.com | bash
exec -l $SHELL
gcloud auth login
gcloud config set project gcp-sftp-transfer

# Python & 必要ライブラリ
sudo apt update && sudo apt install -y python3-pip
pip3 install --upgrade google-cloud-storage paramiko
```

### **1.2 変数の設定（`credentials/credential.json` に保存）**
```json
{
    ＊＊＊
}
```
**🔍 ポイント**
- `SFTP_PRIVATE_KEY_B64` は **秘密鍵を Base64 に変換** して保存
  ```sh
  base64 ~/.ssh/id_rsa > private_key.b64
  cat private_key.b64
  ```
- `credentials/credential.json` は `.gitignore` に追加し、**GITには含めない！**

---

## **2. Cloud Functions のデプロイ**
### **2.1 GCP Cloud Functions のソースコード**
`src/main.py`
```python
import os
import io
import json
import base64
import paramiko
from google.cloud import storage

# 認証情報の読み込み
with open("credentials/credential.json") as f:
    config = json.load(f)

SFTP_HOST = config["SFTP_HOST"]
SFTP_PORT = int(config["SFTP_PORT"])
SFTP_USER = config["SFTP_USER"]
SFTP_PRIVATE_KEY_B64 = config["SFTP_PRIVATE_KEY_B64"]
SFTP_REMOTE_PATH = config["SFTP_REMOTE_PATH"]
GCS_BUCKET_NAME = config["GCS_BUCKET_NAME"]

def upload_to_sftp(file_name, file_content):
    private_key_data = base64.b64decode(SFTP_PRIVATE_KEY_B64)
    private_key = paramiko.RSAKey(file_obj=io.BytesIO(private_key_data))

    transport = paramiko.Transport((SFTP_HOST, SFTP_PORT))
    transport.connect(username=SFTP_USER, pkey=private_key)
    sftp = paramiko.SFTPClient.from_transport(transport)

    remote_file = f"{SFTP_REMOTE_PATH}/{file_name}"
    with sftp.open(remote_file, "wb") as f:
        f.write(file_content.encode())

    sftp.close()
    transport.close()
    print(f"✅ {file_name} を SFTP にアップロード完了")

def gcs_to_sftp(event, context):
    storage_client = storage.Client()
    bucket = storage_client.bucket(GCS_BUCKET_NAME)
    blob = bucket.blob(event["name"])
    file_content = blob.download_as_text()
    
    upload_to_sftp(event["name"], file_content)
```

### **2.2 デプロイ**
```sh
gcloud functions deploy gcp-cf-sftp-transfer \
    --gen2 \
    --runtime python310 \
    --region asia-northeast1 \
    --source src/ \
    --entry-point gcs_to_sftp \
    --trigger-event-filters="type=google.cloud.storage.object.v1.finalized" \
    --trigger-event-filters="bucket=gcp-gcs-bucket" \
    --allow-unauthenticated
```

---

## **3. 動作確認**
### **3.1 SFTP 接続テスト**
```sh
sftp -i ~/.ssh/id_rsa core-sftp-user@s-xxxxxxxxxxxx.server.transfer.ap-northeast-1.amazonaws.com
```
```sftp
sftp> pwd
Remote working directory: /core-system-sftp-bucket/core-sftp-user
sftp> ls
（ファイル一覧が表示される）
sftp> exit
```

### **3.2 GCS → SFTP 転送テスト**
```sh
echo "col1,col2,col3\nvalue1,value2,value3" > test_file.csv
gsutil cp test_file.csv gs://gcp-gcs-bucket/
sleep 10
gcloud functions logs read gcp-cf-sftp-transfer --region=asia-northeast1 --limit=10
sftp -i ~/.ssh/id_rsa core-sftp-user@s-xxxxxxxxxxxx.server.transfer.ap-northeast-1.amazonaws.com <<< $'ls /core-system-sftp-bucket/core-sftp-user'
```
**✅ 成功時の出力**
```
Copying file://test_file.csv [Content-Type=text/csv]...
Operation completed over 1 objects/37.0 B.

LEVEL: 
NAME: gcp-cf-sftp-transfer
EXECUTION_ID: x7dhnISxMaSk
TIME_UTC: 2025-02-23 00:00:08.189
LOG: ✅ test_file.csv を SFTP にアップロード完了

sftp> ls /core-system-sftp-bucket/core-sftp-user
test_file.csv
```

---

## **4. トラブルシューティング**
| **エラー** | **原因** | **解決策** |
|------------|----------|-------------|
| `TypeError: argument should be a bytes-like object or ASCII string, not 'NoneType'` | `SFTP_PRIVATE_KEY_B64` の値が `None` になっている | `.env` を再設定し、関数を再デプロイ |
| `cannot use a string pattern on a bytes-like object` | Base64 デコード後の `private_key_data` を `bytes` に変換していない | `private_key_data.encode()` を削除 |
| `Permission denied (publickey).` | SFTP の鍵が間違っている | `~/.ssh/id_rsa` を確認し、正しい鍵を使用 |
| `Error: Not Found` | GCS でファイルが見つからない | `gsutil ls gs://gcp-gcs-bucket/` でファイルを確認 |

---

## **5. GitHub への保存**
### **5.1 `.gitignore` の設定**
```gitignore
# 認証情報はGit管理しない
credentials/
.env
*.pem
*.json
```

### **5.2 コミット & プッシュ**
```sh
git add .
git commit -m "GCP SFTP transfer function initial commit"
git push origin main
```

---

## **6. 今後の改善点**
✅ AWS SFTP 側のアクセス制御設定  
✅ AWS Transfer Family のロール権限確認  
✅ GCP Cloud Function のエラーハンドリング追加

---

これで **個人用の作業手順** は完了！  
次に **職場用** にクリデンシャルを抜いたバージョンを作成するよ。  
```

---




# GCP → AWS SFTP 転送: 職場用作業手順（パスワード認証パターン）

このドキュメントは **Google Cloud Functions** を利用して、  
**Google Cloud Storage (GCS) にアップロードされたファイルを AWS SFTP に転送する仕組み（パスワード認証パターン）** を構築するための手順をまとめたものです。

---

## **1. 環境セットアップ**
### **1.1 必要なツール**
以下のツールがインストールされていることを確認してください。

- **Google Cloud SDK** (`gcloud` コマンド)
- **Python 3.x** (`python3` コマンド)
- **pip** (`pip3` コマンド)
- **Git** (`git` コマンド)

```sh
# Google Cloud SDK インストール（必要に応じて）
curl https://sdk.cloud.google.com | bash
exec -l $SHELL
gcloud auth login
gcloud config set project <PROJECT_ID>

# 必要なライブラリのインストール
sudo apt update && sudo apt install -y python3-pip
pip3 install --upgrade google-cloud-storage paramiko



**✅ AWS 側が未完成の場合の発火テストについて**

### **1. GCP 側の関数発火は可能か？**
✔ **結論：AWS 側が未完成でも、Cloud Functions の発火テストは可能**  
Cloud Functions は **GCS にファイルがアップロードされたことをトリガー** にして実行されるため、AWS 側の SFTP が未完成でも、  
**GCS にテストファイルをアップロードすれば、関数が発火するかどうかは検証できる。**

### **2. AWS への転送部分をスキップする方法**
もし AWS 側が準備できていない場合、関数の SFTP アップロード部分をコメントアウトして、  
**関数の発火と GCS からのデータ取得が正常に行われるかだけを確認するテストが可能。**

例: `main.py` の `upload_to_sftp` を一時的に無効化
```python
def gcs_to_sftp(event, context):
    storage_client = storage.Client()
    bucket = storage_client.bucket(GCS_BUCKET_NAME)
    blob = bucket.blob(event["name"])
    file_content = blob.download_as_text()

    print(f"✅ GCSからファイル取得成功: {event['name']}")
    print(f"📄 ファイル内容:\n{file_content}")

    # AWS SFTP 転送部分を一時的に無効化
    # upload_to_sftp(event["name"], file_content)
```
✔ **この状態で GCS にファイルをアップロードすると、Cloud Functions が発火し、ログにファイルの内容が表示されるか確認可能。**

### **3. 発火テストの手順**
```sh
# 1. テスト用のファイルを作成
echo "col1,col2,col3\nvalue1,value2,value3" > test_file.csv

# 2. GCS にアップロード
gsutil cp test_file.csv gs://<GCS_BUCKET_NAME>/

# 3. Cloud Functions のログを確認（関数が発火したか）
gcloud functions logs read gcp-cf-sftp-transfer --region=asia-northeast1 --limit=10
```

✔ **ログに `✅ GCSからファイル取得成功` と `📄 ファイル内容` が表示されれば、発火テスト成功！**  
AWS 側が未完成でも、GCP 側の設定・Cloud Functions の動作確認は進められる。






Cloud Functions（CF）から **固定IP（Static IP）を使用してAWS SFTPに接続** するには、 **Cloud NAT** を利用するのが一般的だね。

---

## **GCPでCloud Functionsに固定IPを割り当てる方法**
### **1. 概要**
GCPでは、**Cloud Functions** はデフォルトで **動的な外部IPを使用** している。そのため、**固定IPを使用するにはCloud NATを設定する必要がある**。

#### **手順の概要**
1. **VPC ネットワークの作成**（または既存のものを使用）
2. **Cloud Router の作成**
3. **Cloud NAT の作成**
4. **Cloud Functions の VPC コネクタ設定**

---

## **2. 実装時間の見積もり**
各ステップにかかる目安時間は以下のとおり。

| ステップ | 作業内容 | 目安時間 |
|----------|--------|----------|
| ① VPC ネットワークの作成 | 新規で作成（既存があればスキップ） | **10分** |
| ② Cloud Router の作成 | VPC 内に作成 | **5分** |
| ③ Cloud NAT の作成 | Cloud Router 経由でNATを設定 | **10分** |
| ④ Cloud Functions にVPC コネクタを追加 | Cloud Functions をVPCに接続 | **15~20分** |
| **合計** | **約30～45分**（既存VPCがある場合は20～30分） |

---

## **3. 手順**
### **① VPC ネットワークの作成**
（既存のVPCがある場合はスキップ）

```sh
gcloud compute networks create my-vpc \
    --subnet-mode=custom
```

✅ **確認コマンド**
```sh
gcloud compute networks list
```

---

### **② Cloud Router の作成**
Cloud Router はCloud NATに必要なコンポーネント。

```sh
gcloud compute routers create my-router \
    --network=my-vpc \
    --region=asia-northeast1
```

✅ **確認コマンド**
```sh
gcloud compute routers list
```

---

### **③ Cloud NAT の作成**
固定IPでNATを設定する。

```sh
gcloud compute routers nats create my-nat \
    --router=my-router \
    --region=asia-northeast1 \
    --nat-custom-subnet-ip-ranges=my-subnet
```

✅ **確認コマンド**
```sh
gcloud compute routers nats list
```

---

### **④ Cloud Functions に VPC コネクタを追加**
Cloud Functions がVPC内で動作するようにする。

```sh
gcloud compute networks vpc-access connectors create my-vpc-connector \
    --network=my-vpc \
    --region=asia-northeast1 \
    --range=10.8.0.0/28
```

✅ **確認コマンド**
```sh
gcloud compute networks vpc-access connectors list
```

#### **Cloud Function のデプロイ時にVPCコネクタを指定**
```sh
gcloud functions deploy gcp-cf-sftp-transfer \
    --region=asia-northeast1 \
    --vpc-connector=my-vpc-connector \
    --egress-settings=all
```

✅ **確認コマンド**
```sh
gcloud functions describe gcp-cf-sftp-transfer
```

---

## **4. 結論**
- **新規でVPCを作るなら** → **約45分**
- **既存VPCがあるなら** → **約30分**
- **トラブルシューティング込みで1時間程度を見ておくと安心** ✅

もしVPCを作る必要がないなら、**30分以内に完了** すると思うよ！🚀


